{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "personalized-shooting",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression\n",
    "### 3.1. Model Definition (two class)\n",
    "Transforms continuous value from linear regression into discrete values for output.\n",
    "\n",
    "$$P(Y=1|X=\\{X_1,\\ldots, X_n\\})=\\frac{1}{1+\\exp{(w_0 + \\sum_{i=1}^n w_iX_i)}}$$\n",
    "\n",
    "$$P(Y=0|X=\\{X_1,\\ldots, X_n\\})=\\frac{\\exp{(w_0 + \\sum_{i=1}^n w_iX_i)}}{1+\\exp{(w_0 + \\sum_{i=1}^n w_iX_i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-policy",
   "metadata": {},
   "source": [
    "### 3.2. MCLE (two class)\n",
    "We want to use MCLE to learn the model parameters. MCLE cannot be solved in closed-form.\n",
    "\n",
    "$$\\hat{W}_{MCLE}=\\underset{W}{argmax} \\prod_l P(Y^l|X^l,W)$$\n",
    "\n",
    "where\n",
    "- $l$: number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-cuisine",
   "metadata": {},
   "source": [
    "### 3.3. MCLE (two class) with Gradient Ascent\n",
    "Parameters can be derived using gradient descent because logistic regression is concave. Start with a random initialization of parameters. Repeat until the change is less than $\\epsilon$, that is, until $l(W)_t - l(W)_{t-1} < \\epsilon$.\n",
    "\n",
    "$$l(W)=\\sum_l Y^l(w_0+\\sum_i^n w_i X^l_i)-ln(1+exp(w_0+\\sum_i^n w_i X^l_i))$$\n",
    "\n",
    "$$w_i \\leftarrow w_i + \\eta \\sum_l X^l_i\\left(Y^l-\\hat{P}(Y^l=1|X^l,W)\\right) $$\n",
    "\n",
    "where \n",
    "- $\\eta$ is step size (learning rate)\n",
    "- $ X^l_i\\left(Y^l-\\hat{P}(Y^l=1|X^l,W)\\right)$ is the gradient\n",
    "- $\\hat{P}(Y^l=1|X^l,W)=\\frac{exp(w_0+\\sum_iw_iX_i)}{1+exp(w_0+\\sum_iw_iX_i)}$\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-highland",
   "metadata": {},
   "source": [
    "### 3.4. MAP estimation (two class) with Gradient Ascent\n",
    "Regularization term helps reduce overfitting, especially when training data is sparse.\n",
    "\n",
    "$$l(W)=?$$\n",
    "\n",
    "$$w_i \\leftarrow w_i -\\eta\\lambda w_i + \\eta \\sum_l X^l_i\\left(Y^l-\\hat{P}(Y^l=1|X^l,W)\\right) $$\n",
    "\n",
    "where\n",
    "- $\\lambda$ is a regularization term, $\\lambda=\\frac{1}{2\\sigma^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-tribe",
   "metadata": {},
   "source": [
    "### 3.4. Model Definition (multiclass)\n",
    "Learn R-1 set of weights.\n",
    "\n",
    "For k<R:\n",
    "\n",
    "$$P(Y=y_k|X=\\{X_1,\\ldots, X_n\\})=\\frac{\\exp{(w_{k,0} + \\sum_{i=1}^n w_{k,i}X_i)}}{1+\\sum_{j=1}^{R-1}\\exp{(w_{j,0} + \\sum_{i=1}^n w_{ji}X_i)}}$$\n",
    "\n",
    "where\n",
    "- R: number of classes\n",
    "\n",
    "***\n",
    "\n",
    "For k=R:\n",
    "\n",
    "$$P(Y=y_R|X=\\{X_1,\\ldots, X_n\\})=\\frac{1}{1+\\sum_{j=1}^{R-1}\\exp{(w_{j,0} + \\sum_{i=1}^n w_{ji}X_i)}}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
