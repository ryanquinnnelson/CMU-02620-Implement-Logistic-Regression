{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decreased-basics",
   "metadata": {},
   "source": [
    "# 1. K-nearest Neighbor Classifier\n",
    "### 1.1. Model Definition\n",
    "Non-parametric, no model, no learning.\n",
    "\n",
    "- Given $X_{new}$, find its k-nearest neighbors according to some distance measure\n",
    "    - Euclidean distance: $(X_i-X_{new})^T(X_i-X_{new})$\n",
    "- Classify $X_{new}$ as the majority vote, based on the labels of these neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "overall-emission",
   "metadata": {},
   "source": [
    "# 2. Naïve Bayes Classifier\n",
    "### 2.1. Model Definition\n",
    "Model assumes all features are conditionally independent.\n",
    "\n",
    "$$P(Y|X_1,\\ldots,X_n)=\\frac{P(Y)\\prod_{i=1}^n P(X_i|Y)}{P(X_1,\\dots,X_n)}$$\n",
    "\n",
    "\n",
    "where \n",
    "- $X$ is a feature for a given sample. \n",
    "- $n$ is the total number of samples.\n",
    "- $Y$ is the label for a given sample.\n",
    "\n",
    "\n",
    "#### Subcomponents\n",
    "\n",
    "\n",
    "$P(Y)=\\pi^Y(1-\\pi)^{1-Y}$\n",
    "\n",
    "where \n",
    "- $\\pi$ is a parameter defining $P(Y=1)$ and must be learned.\n",
    "\n",
    "***\n",
    "If $X_i$ is discrete, $P(X_i|Y)$ follows a Multinoulli distribution:\n",
    "\n",
    "$P(X_i=k|Y=j)=\\prod_k \\theta_{ijk}^{(1-\\delta(X_i,k))}$\n",
    "\n",
    "where \n",
    "- $\\delta(X_i,k)$ is an indicator function which takes value 1 if $X_i=k$ and value 0 otherwise.\n",
    "- $k$: The number of possible values for a given feature.\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "If $X_i$ is continuous, $P(X_i|Y)$ follows a Gaussian distribution:\n",
    "\n",
    "$P(X_i|Y)\\sim N(\\mu_i,\\sigma_i^2)$\n",
    "\n",
    "where \n",
    "- $\\mu_i$ and $\\sigma_i^2$ are parameters which must be learned. \n",
    "***\n",
    "$P(X_1,\\dots,X_n)$\n",
    "\n",
    "where\n",
    "\n",
    "$P(X_1,\\dots,X_n)=P(Y=0)\\sum_{i=1}^n P(X_i|Y=0)+P(Y=1)\\sum_{i=1}^n P(X_i|Y=1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-windows",
   "metadata": {},
   "source": [
    "### 2.3. MLE for Naïve Bayes\n",
    "\n",
    "$$\\pi=\\frac{s_1}{n}$$\n",
    "\n",
    "where \n",
    "- $s_1$: number of samples with $Y=1$\n",
    "- $n$: total number of samples\n",
    "\n",
    "***\n",
    "\n",
    "$$\\theta_{ijk}=\\frac{s_{1,k}}{s_1}$$\n",
    "\n",
    "where\n",
    "- $s_{1,k}$: number of samples with $X_i=k,Y=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extended-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given $X_{new}$:\n",
    "# - Compute $P(Y=1)\\prod_{i=1}^nP(X_j=X_{j_{new}}|Y=1)$\n",
    "# - Compute $P(Y=0)\\prod_{i=1}^nP(X_j=X_{j_{new}}|Y=0)$\n",
    "# - Classify $X_{new}$ as the label with the higher probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-escape",
   "metadata": {},
   "source": [
    "### 2.4. MAP estimation for Naïve Bayes\n",
    "\n",
    "$$\\pi=\\frac{s_1+\\alpha_0}{n+\\alpha_0 + \\beta_0}$$\n",
    "\n",
    "where \n",
    "- $s_1$: number of samples with $Y=1$\n",
    "- $n$: total number of samples\n",
    "- $\\alpha_0$:\n",
    "- $\\beta_0$:\n",
    "\n",
    "***\n",
    "\n",
    "$$\\theta_{ijk}=\\frac{s_{1,k}+\\alpha_{ijk0}}{s_1+\\alpha_{ijk0}+\\beta_{ijk0}}$$\n",
    "\n",
    "where\n",
    "- $s_{1,k}$: number of samples with $X_i=k,Y=1$\n",
    "- $\\alpha_{ijk0}$:\n",
    "- $\\beta_{ijk0}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "amended-circulation",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression\n",
    "### 3.1. Model Definition (two class)\n",
    "Transforms continuous value from linear regression into discrete values for output.\n",
    "\n",
    "$$P(Y=1|X=\\{X_1,\\ldots, X_n\\})=\\frac{1}{1+\\exp{(w_0 + \\sum_{i=1}^n w_iX_i)}}$$\n",
    "\n",
    "$$P(Y=0|X=\\{X_1,\\ldots, X_n\\})=\\frac{\\exp{(w_0 + \\sum_{i=1}^n w_iX_i)}}{1+\\exp{(w_0 + \\sum_{i=1}^n w_iX_i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-container",
   "metadata": {},
   "source": [
    "### 3.2. MCLE for Logistic Regression (two class)\n",
    "We want to use MCLE to learn the model parameters. MCLE cannot be solved in closed-form with respect to $W$.\n",
    "\n",
    "$$\\hat{W}_{MCLE}=\\underset{W}{argmax} \\prod_l P(Y^l|X^l,W)$$\n",
    "\n",
    "where\n",
    "- $l$: number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-omaha",
   "metadata": {},
   "source": [
    "### 3.3. MCLE  for Logistic Regression (two class) with Gradient Descent\n",
    "Parameters can be derived using gradient descent because logistic regression is concave. Start with a random initialization of parameters. Repeat until the change is less than $\\epsilon$, that is, until $l(W)_t - l(W)_{t-1} < \\epsilon$.\n",
    "\n",
    "$$l(W)=\\sum_l Y^l(w_0+\\sum_i^n w_i X^l_i)-ln(1+exp(w_0+\\sum_i^n w_i X^l_i))$$\n",
    "\n",
    "$$w_i \\leftarrow w_i + \\eta \\nabla(W) $$\n",
    "\n",
    "where \n",
    "- $\\eta$ is step size (learning rate)\n",
    "- $X_i^l$: value of $X_i$ for the $l$th training example.\n",
    "- $\\nabla(W)$ is the gradient\n",
    "\n",
    "***\n",
    "\n",
    "#### Subcomponents\n",
    "\n",
    "$\\nabla(W)=\\frac{\\partial l(W)}{\\partial w_i}=\\sum_l X^l_i\\left(Y^l-\\hat{P}(Y^l=1|X^l,W)\\right)$\n",
    "\n",
    "where\n",
    "- $Y^l-\\hat{P}(Y^l=1|X^l,W)$ is the prediction error\n",
    "- $\\hat{P}(Y^l=1|X^l,W)=\\frac{exp(w_0+\\sum_iw_iX_i)}{1+exp(w_0+\\sum_iw_iX_i)}$\n",
    "\n",
    "\n",
    "According to Mitchell, we accommodate weight $w_0$ by assuming an imaginary $X_0=1$ for all $l$.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-theology",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial l(W)}{\\partial w_0}=\\sum_l 1 (Y^l - P(Y^l=1|X^l,W))$$\n",
    "\n",
    "$$\\frac{\\partial l(W)}{\\partial w_1}=\\sum_l X_1^l (Y^l - P(Y^l=1|X^l,W))$$\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "$$\\frac{\\partial l(W)}{\\partial w_n}=\\sum_l X_n^l (Y^l - P(Y^l=1|X^l,W))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?? way to mean center data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "moved-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y_true,eta,epsilon,W):\n",
    "    '''\n",
    "    Performs gradient descent to derive regression coefficients.\n",
    "    '''\n",
    "    prev_log_likelihood = calc_log_likelihood(X,y,W)\n",
    "    \n",
    "    diff = np.Inf\n",
    "    while diff > epsilon:\n",
    "    \n",
    "        # update weights\n",
    "        y_pred = get_y_predictions(X,W)\n",
    "        gradient = calc_gradient(X,y_true,y_pred)\n",
    "        W = update_weights(W,eta,gradient)\n",
    "\n",
    "        # calculate difference\n",
    "        log_likelihood = calc_log_likelihood(X,y,W)\n",
    "        diff = prev_log_likelihood - log_likelihood\n",
    "    \n",
    "    return W    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "oriented-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_likelihood(X,y,W):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "royal-horizontal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test get_y_predictions()\n"
     ]
    }
   ],
   "source": [
    "def get_y_predictions(X,W):\n",
    "\n",
    "    a = W[0] + np.dot(X,W[1:])\n",
    "    b = exp(a)\n",
    "    c = 1 + exp(a)\n",
    "\n",
    "    return b/c\n",
    "\n",
    "print('test get_y_predictions()')\n",
    "X = np.array([1,2])\n",
    "W = np.array([4,5,6])\n",
    "expected = 1318815734 / 1318815735\n",
    "actual = get_y_predictions(X,W)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "secret-nursing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test calc_gradient()\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "#?? how to accommodate w_0? Assume extra x row of all 1's?\n",
    "def calc_gradient(X,y_true,y_pred):\n",
    "    '''\n",
    "    Calculates the gradient.\n",
    "    \n",
    "    Args:\n",
    "        X: L x n matrix, where L is the number of samples and n is the number of features\n",
    "        y_true: L x 1 vector\n",
    "        y_pred: L x 1 vector\n",
    "        \n",
    "    Return:\n",
    "        Gradient in the form of an L x 1 vector\n",
    "    '''\n",
    "    y_err = y_true - y_pred\n",
    "    print(np.matmul(X.T,y_err).shape)\n",
    "    return np.matmul(X.T,y_err)\n",
    "\n",
    "'''\n",
    "X                y_err\n",
    "| 5 | 1 | 1 |    | 0 |\n",
    "| 1 | 1 | 1 |    |-1 |\n",
    "| 1 | 2 | 3 |    | 1 |\n",
    "\n",
    "Calculating the partial for the ith feature is the same as taking the dot product of the ith column of X and y_err.\n",
    "\n",
    "X_1         y_err\n",
    "|x11|       | y1 | \n",
    "|x21|  dot  | y2 | \n",
    "|x31|       | y3 |\n",
    "\n",
    "w_1 = (x11*y1) + (x21*y2) + (x31*y3)\n",
    "\n",
    "X_2         y_err\n",
    "|x12|       | y1 | \n",
    "|x22|  dot  | y2 | \n",
    "|x32|       | y3 |\n",
    "\n",
    "w_2 = (x12*y1) + (x22*y2) + (x32*y3)\n",
    "\n",
    "X_3         y_err\n",
    "|x13|       | y1 | \n",
    "|x23|  dot  | y2 | \n",
    "|x33|       | y3 |\n",
    "\n",
    "w_3 = (x13*y1) + (x23*y2) + (x33*y3)\n",
    "\n",
    "Instead of calculating partials one by one, we can perform matrix multiplication to do all at the same time. \n",
    "However, we need to transpose X first, because we want to perform column-wise multiplications.\n",
    "\n",
    "X              y_err\n",
    "|x11|x12|x13|  | y1 | \n",
    "|x21|x22|x23|  | y2 |\n",
    "|x31|x32|x33|  | y3 |\n",
    "\n",
    "(X)(y_err)\n",
    "| (x11*y1) + (x12*y2) + (x13*y3) |\n",
    "| (x21*y1) + (x22*y2) + (x23*y3) |\n",
    "| (x31*y1) + (x32*y2) + (x33*y3) |\n",
    "\n",
    "X_transpose    y_err\n",
    "|x11|x21|x31|  | y1 | \n",
    "|x12|x22|x32|  | y2 |\n",
    "|x13|x23|x33|  | y3 |\n",
    "\n",
    "W = (X_transpose)(y_err)\n",
    "| w1 |    | (x11*y1) + (x21*y2) + (x31*y3) |\n",
    "| w2 | =  | (x12*y1) + (x22*y2) + (x32*y3) |\n",
    "| w3 |    | (x13*y1) + (x23*y2) + (x33*y3) |\n",
    "'''\n",
    "\n",
    "print('test calc_gradient()')\n",
    "X = np.array([[ 5, 1 ,1], \n",
    "              [ 1, 1 ,1], \n",
    "              [ 1, 2 ,3]])\n",
    "y_true = np.array([1, 0, 1])\n",
    "y_pred = np.array([1, 1, 0])\n",
    "expected = np.array([0, 1, 2]).tolist()\n",
    "actual = calc_gradient(X,y_true,y_pred).tolist()\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "general-tractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test calc_partial()\n",
      "test calc_partial()\n"
     ]
    }
   ],
   "source": [
    "def calc_partial(X,y_true,y_pred):\n",
    "    \n",
    "    y_err = y_true - y_pred\n",
    "    Xy = X * y_err\n",
    "    return np.sum(Xy,axis=0)\n",
    "\n",
    "print('test calc_partial()')\n",
    "X = np.array([1,1,2])\n",
    "y_true = np.array([4,5,6])\n",
    "y_pred = np.array([4,5,6])\n",
    "expected = 0.0\n",
    "actual = calc_partial(X,y_true,y_pred)\n",
    "assert actual == expected, actual\n",
    "\n",
    "print('test calc_partial()')\n",
    "X = np.array([1,1,2])\n",
    "y_true = np.array([4,5,6])\n",
    "y_pred = np.array([3,6,7])\n",
    "expected = -2.0\n",
    "actual = calc_partial(X,y_true,y_pred)\n",
    "assert actual == expected, actual\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "angry-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, eta, epsilon):\n",
    "        '''\n",
    "        Args:\n",
    "            eta: learning rate\n",
    "            epsilon: convergence threshold\n",
    "        '''\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(X,y):\n",
    "        \n",
    "        # generate random weights for each feature\n",
    "        weights = np.array([])\n",
    "\n",
    "        # perform gradient descent until convergence\n",
    "        weights = gradient_descent(X,y,eta,epsilon,weights)\n",
    "\n",
    "        self.weights = weights\n",
    "        \n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-pierce",
   "metadata": {},
   "source": [
    "### 3.4. MAP estimation  for Logistic Regression (two class) with Gradient Descent\n",
    "Regularization term helps reduce overfitting, especially when training data is sparse.\n",
    "\n",
    "$$l(W)=?$$\n",
    "\n",
    "$$w_i \\leftarrow w_i -\\eta\\lambda w_i + \\eta \\sum_l X^l_i\\left(Y^l-\\hat{P}(Y^l=1|X^l,W)\\right) $$\n",
    "\n",
    "where\n",
    "- $\\lambda$ is a regularization term, $\\lambda=\\frac{1}{2\\sigma^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-apache",
   "metadata": {},
   "source": [
    "### 3.5. Model Definition (multiclass)\n",
    "Logistic Regression for more than two classes. Learn $R-1$ set of weights.\n",
    "\n",
    "For $k<R$:\n",
    "\n",
    "$$P(Y=y_k|X=\\{X_1,\\ldots, X_n\\})=\\frac{\\exp{(w_{k,0} + \\sum_{i=1}^n w_{k,i}X_i)}}{1+\\sum_{j=1}^{R-1}\\exp{(w_{j,0} + \\sum_{i=1}^n w_{ji}X_i)}}$$\n",
    "\n",
    "where\n",
    "- $R$: number of classes\n",
    "\n",
    "***\n",
    "\n",
    "For $k=R$:\n",
    "\n",
    "$$P(Y=y_R|X=\\{X_1,\\ldots, X_n\\})=\\frac{1}{1+\\sum_{j=1}^{R-1}\\exp{(w_{j,0} + \\sum_{i=1}^n w_{ji}X_i)}}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
