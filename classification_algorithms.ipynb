{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "presidential-honey",
   "metadata": {},
   "source": [
    "# 1. K-nearest Neighbor Classifier\n",
    "### 1.1. Model Definition\n",
    "Non-parametric, no model, no learning.\n",
    "\n",
    "- Given $X_{new}$, find its k-nearest neighbors according to some distance measure\n",
    "    - Euclidean distance: $(X_i-X_{new})^T(X_i-X_{new})$\n",
    "- Classify $X_{new}$ as the majority vote, based on the labels of these neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-malpractice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "convinced-distance",
   "metadata": {},
   "source": [
    "# 2. Naïve Bayes Classifier\n",
    "### 2.1. Model Definition\n",
    "Model assumes all features are conditionally independent.\n",
    "\n",
    "$$P(Y|X_1,\\ldots,X_n)=\\frac{P(Y)\\prod_{i=1}^n P(X_i|Y)}{P(X_1,\\dots,X_n)}$$\n",
    "\n",
    "\n",
    "where \n",
    "- $X$ is a feature for a given sample. \n",
    "- $n$ is the total number of samples.\n",
    "- $Y$ is the label for a given sample.\n",
    "\n",
    "\n",
    "#### Subcomponents\n",
    "\n",
    "\n",
    "$P(Y)=\\pi^Y(1-\\pi)^{1-Y}$\n",
    "\n",
    "where \n",
    "- $\\pi$ is a parameter defining $P(Y=1)$ and must be learned.\n",
    "\n",
    "***\n",
    "If $X_i$ is discrete, $P(X_i|Y)$ follows a Multinoulli distribution:\n",
    "\n",
    "$P(X_i=k|Y=j)=\\prod_k \\theta_{ijk}^{(1-\\delta(X_i,k))}$\n",
    "\n",
    "where \n",
    "- $\\delta(X_i,k)$ is an indicator function which takes value 1 if $X_i=k$ and value 0 otherwise.\n",
    "- $k$: The number of possible values for a given feature.\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "If $X_i$ is continuous, $P(X_i|Y)$ follows a Gaussian distribution:\n",
    "\n",
    "$P(X_i|Y)\\sim N(\\mu_i,\\sigma_i^2)$\n",
    "\n",
    "where \n",
    "- $\\mu_i$ and $\\sigma_i^2$ are parameters which must be learned. \n",
    "***\n",
    "$P(X_1,\\dots,X_n)$\n",
    "\n",
    "where\n",
    "\n",
    "$P(X_1,\\dots,X_n)=P(Y=0)\\sum_{i=1}^n P(X_i|Y=0)+P(Y=1)\\sum_{i=1}^n P(X_i|Y=1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-faith",
   "metadata": {},
   "source": [
    "### 2.3. MLE for Naïve Bayes\n",
    "\n",
    "$$\\pi=\\frac{s_1}{n}$$\n",
    "\n",
    "where \n",
    "- $s_1$: number of samples with $Y=1$\n",
    "- $n$: total number of samples\n",
    "\n",
    "***\n",
    "\n",
    "$$\\theta_{ijk}=\\frac{s_{1,k}}{s_1}$$\n",
    "\n",
    "where\n",
    "- $s_{1,k}$: number of samples with $X_i=k,Y=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "forward-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given $X_{new}$:\n",
    "# - Compute $P(Y=1)\\prod_{i=1}^nP(X_j=X_{j_{new}}|Y=1)$\n",
    "# - Compute $P(Y=0)\\prod_{i=1}^nP(X_j=X_{j_{new}}|Y=0)$\n",
    "# - Classify $X_{new}$ as the label with the higher probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-northeast",
   "metadata": {},
   "source": [
    "### 2.4. MAP estimation for Naïve Bayes\n",
    "\n",
    "$$\\pi=\\frac{s_1+\\alpha_0}{n+\\alpha_0 + \\beta_0}$$\n",
    "\n",
    "where \n",
    "- $s_1$: number of samples with $Y=1$\n",
    "- $n$: total number of samples\n",
    "- $\\alpha_0$:\n",
    "- $\\beta_0$:\n",
    "\n",
    "***\n",
    "\n",
    "$$\\theta_{ijk}=\\frac{s_{1,k}+\\alpha_{ijk0}}{s_1+\\alpha_{ijk0}+\\beta_{ijk0}}$$\n",
    "\n",
    "where\n",
    "- $s_{1,k}$: number of samples with $X_i=k,Y=1$\n",
    "- $\\alpha_{ijk0}$:\n",
    "- $\\beta_{ijk0}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-hampton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "modified-latvia",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression\n",
    "### 3.1. Model Definition (two class)\n",
    "Transforms continuous value from linear regression into discrete values for output.\n",
    "\n",
    "$$P(Y=1|X=\\{X_1,\\ldots, X_n\\})=\\frac{1}{1+\\exp{(w_0 + \\sum_{i=1}^n w_iX_i)}}$$\n",
    "\n",
    "$$P(Y=0|X=\\{X_1,\\ldots, X_n\\})=\\frac{\\exp{(w_0 + \\sum_{i=1}^n w_iX_i)}}{1+\\exp{(w_0 + \\sum_{i=1}^n w_iX_i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-press",
   "metadata": {},
   "source": [
    "### 3.2. MCLE for Logistic Regression (two class)\n",
    "We want to use MCLE to learn the model parameters. MCLE cannot be solved in closed-form with respect to $W$.\n",
    "\n",
    "$$\\hat{W}_{MCLE}=\\underset{W}{argmax} \\prod_l P(Y^l|X^l,W)$$\n",
    "\n",
    "where\n",
    "- $l$: number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-pickup",
   "metadata": {},
   "source": [
    "### 3.3. MCLE  for Logistic Regression (two class) with Gradient Descent\n",
    "Parameters can be derived using gradient descent because logistic regression is concave. Start with a random initialization of parameters. Repeat until the change is less than $\\epsilon$, that is, until $l(W)_t - l(W)_{t-1} < \\epsilon$.\n",
    "\n",
    "$$l(W)=\\sum_l Y^l(w_0+\\sum_i^n w_i X^l_i)-ln(1+exp(w_0+\\sum_i^n w_i X^l_i))$$\n",
    "\n",
    "$$w_i \\leftarrow w_i + \\eta \\nabla(W) $$\n",
    "\n",
    "where \n",
    "- $\\eta$ is step size (learning rate)\n",
    "- $X_i^l$: value of $X_i$ for the $l$th training example.\n",
    "- $\\nabla(W)$ is the gradient\n",
    "\n",
    "***\n",
    "\n",
    "#### Subcomponents\n",
    "\n",
    "$\\nabla(W)=\\frac{\\partial l(W)}{\\partial w_i}=\\sum_l X^l_i\\left(Y^l-\\hat{P}(Y^l=1|X^l,W)\\right)$\n",
    "\n",
    "where\n",
    "- $Y^l-\\hat{P}(Y^l=1|X^l,W)$ is the prediction error\n",
    "- $\\hat{P}(Y^l=1|X^l,W)=\\frac{exp(w_0+\\sum_iw_iX_i)}{1+exp(w_0+\\sum_iw_iX_i)}$\n",
    "\n",
    "\n",
    "According to Mitchell, we accommodate weight $w_0$ by assuming an imaginary $X_0=1$ for all $l$.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, eta, epsilon):\n",
    "        self.eta = eta\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def calc_prediction(X,y,weights):\n",
    "        pass\n",
    "\n",
    "eta,epsilon\n",
    "\n",
    "    def gradient_descent(X,y,eta,epsilon,weights):\n",
    "        pass\n",
    "\n",
    "    def fit(X,y,):\n",
    "        # generate random weights for each feature\n",
    "        weights = np.array([])\n",
    "\n",
    "        # perform gradient descent until convergence\n",
    "        weights = gradient_descent(X,y,eta,epsilon,weights)\n",
    "\n",
    "        return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-marine",
   "metadata": {},
   "source": [
    "### 3.4. MAP estimation  for Logistic Regression (two class) with Gradient Descent\n",
    "Regularization term helps reduce overfitting, especially when training data is sparse.\n",
    "\n",
    "$$l(W)=?$$\n",
    "\n",
    "$$w_i \\leftarrow w_i -\\eta\\lambda w_i + \\eta \\sum_l X^l_i\\left(Y^l-\\hat{P}(Y^l=1|X^l,W)\\right) $$\n",
    "\n",
    "where\n",
    "- $\\lambda$ is a regularization term, $\\lambda=\\frac{1}{2\\sigma^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-scene",
   "metadata": {},
   "source": [
    "### 3.5. Model Definition (multiclass)\n",
    "Logistic Regression for more than two classes. Learn $R-1$ set of weights.\n",
    "\n",
    "For $k<R$:\n",
    "\n",
    "$$P(Y=y_k|X=\\{X_1,\\ldots, X_n\\})=\\frac{\\exp{(w_{k,0} + \\sum_{i=1}^n w_{k,i}X_i)}}{1+\\sum_{j=1}^{R-1}\\exp{(w_{j,0} + \\sum_{i=1}^n w_{ji}X_i)}}$$\n",
    "\n",
    "where\n",
    "- $R$: number of classes\n",
    "\n",
    "***\n",
    "\n",
    "For $k=R$:\n",
    "\n",
    "$$P(Y=y_R|X=\\{X_1,\\ldots, X_n\\})=\\frac{1}{1+\\sum_{j=1}^{R-1}\\exp{(w_{j,0} + \\sum_{i=1}^n w_{ji}X_i)}}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
